{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics\n",
    "\n",
    "## validation of dependencies\n",
    "\n",
    "Look into the [labs](labs) folder. The readme contains detailed information. Simply make sure docker has started. Then run:\n",
    "\n",
    "```bash\n",
    "make notebook\n",
    "```\n",
    "\n",
    "Then open a browser and go to: [http://localhost:8888/lab](http://localhost:8888/lab)\n",
    "It will ask you for a token. The token can be retrieved from the console:\n",
    "\n",
    "```bash\n",
    "To access the notebook, open this file in a browser:\n",
    "    file:///.../Jupyter/runtime/nbserver-52250-open.html\n",
    "Or copy and paste one of these URLs:\n",
    "    http://localhost:8888/?token=<<token>>\n",
    "```\n",
    "\n",
    "## why python for data science?\n",
    "\n",
    "- High-level (easy to learn)\n",
    "    - In fact more abstraction makes things faster for both the developer and the machine (unlike other programming languages. Example numpy)\n",
    "- Interpreted / Scripting Lang. (speedy development, slow(er) execution)\n",
    "- General Purpose\n",
    "- Multi-Paradigm (OO, Functional, Procedural)\n",
    "- Open-Source (Free!, Thriving Community, Regular Releases)\n",
    "    - Almost every deep learning project has a native python API\n",
    "    - A lot of great frameworks like pyspark, dask, pyTorch, Airflow\n",
    "- Duck Typing\n",
    "    - Allows for fast experimentation (no rigid class hierarchy)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## interactive python\n",
    "- using the REPL\n",
    "- a bigger project (text editor of choice)\n",
    "- (jupyter) notebooks (there also is the jupter lab available at `jupyter lab`\n",
    "    - colorful\n",
    "    - caching of results\n",
    "    - E2E documentation in the direction of the workflow\n",
    "    - magics\n",
    "    \n",
    "    \n",
    "### jupyter basics:\n",
    "- magics https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.03-Magic-Commands.ipynb\n",
    "- help https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.01-Help-And-Documentation.ipynb\n",
    "- shell commands https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.05-IPython-And-Shell-Commands.ipynb\n",
    "- timing code https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/01.07-Timing-and-Profiling.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T19:00:05.134493Z",
     "start_time": "2019-09-01T19:00:04.997460Z"
    }
   },
   "outputs": [],
   "source": [
    "%ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some first steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "square??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit sum(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VS Code\n",
    "\n",
    "https://code.visualstudio.com/docs/python/jupyter-support\n",
    "\n",
    "- sharing of screen with team members (https://visualstudio.microsoft.com/de/services/live-share/?rr=https%3A%2F%2Fcode.visualstudio.com%2Fdocs%2Fpython%2Fjupyter-support)\n",
    "- remote jupyter notebooks (https://blog.ouseful.info/2019/02/11/connecting-to-a-remote-jupyter-notebook-server-running-on-digital-ocean-from-microsoft-vs-code/)\n",
    "- potentially easier to version control\n",
    "- better auto completion\n",
    "\n",
    "\n",
    "Example:\n",
    "- open VSCode for the `lab` folder\n",
    "- play with some basic cells\n",
    "\n",
    "### Google colaboratory\n",
    "- easy collaboration like google docs\n",
    "- separation of compute and storage (you need to mount an object store or google drive)\n",
    "- free GPU\n",
    "\n",
    "https://colab.research.google.com/notebooks/welcome.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## famous python libraries for data science\n",
    "- jupyter (interactive notebooks) like this one\n",
    "- pandas (Excel)\n",
    "- numpy (MatLab, matrices)\n",
    "- matplotlib (visualization)\n",
    "- scikit-learn (machine learning)\n",
    "\n",
    "Great reading: https://github.com/jakevdp/PythonDataScienceHandbook with lots of examples.\n",
    "\n",
    "The examples below are based on https://github.com/jakevdp/PythonDataScienceHandbook though they might be adapted.\n",
    "\n",
    "> When working for your own it is important to read the documentation or further examples.\n",
    "The content below will explain with miniam examples some of the most important topics.\n",
    "\n",
    "### numpy\n",
    "\n",
    "- efficient storage & manipulation of array or matrix like data\n",
    "- you can specify the type of data (int vs. int8, vs. int23, ...) https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/02.01-Understanding-Data-Types.ipynb\n",
    "- building block for a lot of libraries\n",
    "- behaves fairly similar to MatLab from a syntactical perspective regarding indexing and slicing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.zeros(10, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3x3 array of normally distributed random values\n",
    "# with mean 0 and standard deviation 1\n",
    "a = np.random.normal(0, 1, (3, 3))\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "basic properties of numpy arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"ndim: {a.ndim}\")\n",
    "print(f\"shape: {a.shape}\")\n",
    "print(f\"size: {a.size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accessing the contents of the array (indexing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "slicing: `x[start:stop:step]`\n",
    "\n",
    "> NOTE: sub arrays are no-copy views. If the underlying data is manipulated, the original data is manipulated as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a[0:2, [2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = np.random.randint(10, size=(3, 4))  # Two-dimensional array\n",
    "print(x2)\n",
    "x2_sub = x2[:2, :2]\n",
    "print(x2_sub)\n",
    "\n",
    "x2_sub[0, 0] = 99\n",
    "print(x2_sub)\n",
    "\n",
    "x2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pandas\n",
    "\n",
    "good for tabular data. Fairly similar to Excel. Heavily uses `numpy` internally.\n",
    "\n",
    "It is based on Series:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.Series([0.25, 0.5, 0.75, 1.0])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "their underlying numpy array can be accessed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and it has an index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "indexing works very similar to numpy, but more generalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[1:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data > 0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area_dict = {'California': 423967, 'Texas': 695662, 'New York': 141297,\n",
    "             'Florida': 170312, 'Illinois': 149995}\n",
    "area = pd.Series(area_dict)\n",
    "display(area)\n",
    "\n",
    "population_dict = {'California': 38332521,\n",
    "                   'Texas': 26448193,\n",
    "                   'New York': 19651127,\n",
    "                   'Florida': 19552860,\n",
    "                   'Illinois': 12882135}\n",
    "population = pd.Series(population_dict)\n",
    "display(population)\n",
    "\n",
    "states = pd.DataFrame({'population': population,\n",
    "                       'area': area})\n",
    "states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "compute new attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states['density'] = states.population / states.area\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposition is possible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.iloc[:3, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More interesting: boolean indexing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.loc[states.density > 100][['population', 'density']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "setting values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.iloc[0, 2] = 90\n",
    "states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can always apply any numpy function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.log1p(states.population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-01T19:08:41.603945Z",
     "start_time": "2019-09-01T19:08:41.594269Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'foo':[1,2,3], 'bar':['one', 'two', 'three']})\n",
    "# df = pd.read_csv()\n",
    "# df = pd.read_excel()\n",
    "\n",
    "# Parquet is a compressed columnar file format.\n",
    "# - explicit schema\n",
    "# - column pruning (only required columns can be loaded)\n",
    "# df = pd.read_parquet()\n",
    "\n",
    "\n",
    "print(df.dtypes)\n",
    "print(df.isnull().sum())\n",
    "display(df.head())\n",
    "display(df[df.foo > 2])\n",
    "df.foo.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.bar.value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.foo.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GroupBy: Split, Apply, Combine\n",
    "\n",
    "Simple aggregations can give you a flavor of your dataset, but often we would prefer to aggregate conditionally on some label or index: this is implemented in the so-called ``groupby`` operation.\n",
    "The name \"group by\" comes from a command in the SQL database language, but it is perhaps more illuminative to think of it in the terms first coined by Hadley Wickham of Rstats fame: *split, apply, combine*.\n",
    "\n",
    "### Split, apply, combine\n",
    "\n",
    "This makes clear what the ``groupby`` accomplishes:\n",
    "\n",
    "- The *split* step involves breaking up and grouping a ``DataFrame`` depending on the value of the specified key.\n",
    "- The *apply* step involves computing some function, usually an aggregate, transformation, or filtering, within the individual groups.\n",
    "- The *combine* step merges the results of these operations into an output array.\n",
    "\n",
    "While this could certainly be done manually using some combination of the masking, aggregation, and merging commands covered earlier, an important realization is that *the intermediate splits do not need to be explicitly instantiated*. Rather, the ``GroupBy`` can (often) do this in a single pass over the data, updating the sum, mean, count, min, or other aggregate for each group along the way.\n",
    "The power of the ``GroupBy`` is that it abstracts away these steps: the user need not think about *how* the computation is done under the hood, but rather thinks about the *operation as a whole*.\n",
    "\n",
    "As a concrete example, let's take a look at using Pandas for the computation shown in this diagram.\n",
    "We'll start by creating the input ``DataFrame``:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE split (=no split at all)\n",
    "states.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states.population.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'key': ['A', 'B', 'C', 'A', 'B', 'C'],\n",
    "                   'data': range(6)}, columns=['key', 'data'])\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic split-apply-combine operation can be computed with the ``groupby()`` method of ``DataFrame``s, passing the name of the desired key column:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that what is returned is not a set of ``DataFrame``s, but a ``DataFrameGroupBy`` object.\n",
    "This object is where the magic is: you can think of it as a special view of the ``DataFrame``, which is poised to dig into the groups but does no actual computation until the aggregation is applied.\n",
    "This \"lazy evaluation\" approach means that common aggregates can be implemented very efficiently in a way that is almost transparent to the user.\n",
    "\n",
    "To produce a result, we can apply an aggregate to this ``DataFrameGroupBy`` object, which will perform the appropriate apply/combine steps to produce the desired result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('key').sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dispatch methods\n",
    "\n",
    "Through some Python class magic, any method not explicitly implemented by the ``GroupBy`` object will be passed through and called on the groups, whether they are ``DataFrame`` or ``Series`` objects.\n",
    "For example, you can use the ``describe()`` method of ``DataFrame``s to perform a set of aggregations that describe each group in the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('key')['data'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Datasets: Merge and Join. \n",
    "\n",
    "Similar to SQL again. Check the documentation to see all available options (kind of join, specification of keys, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame({'employee': ['Bob', 'Jake', 'Lisa', 'Sue'],\n",
    "                    'group': ['Accounting', 'Engineering', 'Engineering', 'HR']})\n",
    "df2 = pd.DataFrame({'employee': ['Lisa', 'Bob', 'Jake', 'Sue'],\n",
    "                    'hire_date': [2004, 2008, 2012, 2014]})\n",
    "display(df1, df2)\n",
    "\n",
    "df3 = pd.merge(df1, df2)\n",
    "df3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Tables reshaping\n",
    "wide-to-long and the reverse\n",
    "\n",
    "Read-along: https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.09-Pivot-Tables.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns; sns.set()\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "\n",
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.pivot_table('survived', index='sex', columns='class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age = pd.cut(titanic['age'], [0, 18, 80])\n",
    "titanic.pivot_table('survived', ['sex', age], 'class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shell command to download the data:\n",
    "# uncomment and run!\n",
    "#!curl -O https://raw.githubusercontent.com/jakevdp/data-CDCbirths/master/births.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births = pd.read_csv('births.csv')\n",
    "births.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "births['decade'] = 10 * (births['year'] // 10)\n",
    "births.pivot_table('births', index='decade', columns='gender', aggfunc='sum')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "sns.set()  # use Seaborn styles\n",
    "\n",
    "\n",
    "births.pivot_table('births', index='year', columns='gender', aggfunc='sum').plot()\n",
    "plt.ylabel('total births per year');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise:\n",
    "\n",
    "Read the CSV file `data/reshape_me.csv` to a pandas data frame. Then reshape it so that the result looks like:\n",
    "\n",
    "```\n",
    "              value2             value_1            \n",
    "target             0       1           0           1\n",
    "foo group                                           \n",
    "2   bar    2519413.0     NaN  696.454798         NaN\n",
    "3   bar          NaN  3512.0         NaN  656.505657\n",
    "```\n",
    "\n",
    "to move the observations per row into a single row with multiple columns for each target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../solutions/01_reshape.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a JSON string parse it to json and load it to pandas. Normalize the result to be clean & tidy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = \"\"\"\n",
    "[{\n",
    "    \"load\": 1,\n",
    "    \"results\": {\n",
    "        \"key\": \"A\",\n",
    "        \"timing\": 1.1\n",
    "    }\n",
    "}, {\n",
    "    \"load\": 2,\n",
    "    \"results\": {\n",
    "        \"key\": \"B\",\n",
    "        \"timing\": 2.2\n",
    "    }\n",
    "}]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../solutions/01_normalize.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Further topics:\n",
    "- gpu-based pandas https://rapids.ai/index.html\n",
    "- time series: https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/03.11-Working-with-Time-Series.ipynb\n",
    "- visualization in seaborn https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/04.14-Visualization-With-Seaborn.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automate the boring things.\n",
    "Basic data exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas_profiling\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    np.random.rand(100, 5),\n",
    "    columns=['a', 'b', 'c', 'd', 'e']\n",
    ")\n",
    "display(df.head())\n",
    "\n",
    "profile = df.profile_report()\n",
    "rejected_variables = profile.get_rejected_variables(threshold=0.9)\n",
    "\n",
    "print(rejected_variables)\n",
    "\n",
    "profile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## minimalistic web scraping\n",
    "\n",
    "using tabular data: http://www.nationmaster.com/country-info/stats/Media/Internet-users from HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "res = requests.get(\"http://www.nationmaster.com/country-info/stats/Media/Internet-users\")\n",
    "soup = BeautifulSoup(res.content,'lxml')\n",
    "table = soup.find_all('table')[0] \n",
    "df = pd.read_html(str(table))\n",
    "df = df[0]\n",
    "display(df.head()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embeddings\n",
    "\n",
    "Read more: https://github.com/jakevdp/PythonDataScienceHandbook/blob/master/notebooks/05.10-Manifold-Learning.ipynb\n",
    "\n",
    "1. Great reduce the dimensionality and quickly see if groups form.\n",
    "2. some ML models prefer data with lower dimensionality (https://umap-learn.readthedocs.io/en/latest/clustering.html)\n",
    "\n",
    "\n",
    "Uniform Manifold Approximation and Projection (UMAP) is a non-linear dimension reduction technique that can be used for visualisation similarly to t-SNE, but also for general non-linear dimension reduction. (https://github.com/lmcinnes/umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.plotting import figure\n",
    "from bokeh.models import CategoricalColorMapper, ColumnDataSource, FactorRange\n",
    "from bokeh.palettes import Category10\n",
    "from bokeh.io import show, output_notebook\n",
    "\n",
    "output_notebook()\n",
    "\n",
    "import umap\n",
    "from sklearn.datasets import load_digits\n",
    "\n",
    "\n",
    "digits = load_digits()\n",
    "embedding = umap.UMAP().fit_transform(digits.data)\n",
    "\n",
    "targets = [str(d) for d in digits.target_names]\n",
    "\n",
    "source = ColumnDataSource(dict(\n",
    "    x = [e[0] for e in embedding],\n",
    "    y = [e[1] for e in embedding],\n",
    "    label = [targets[d] for d in digits.target]\n",
    "))\n",
    "\n",
    "cmap = CategoricalColorMapper(factors=targets, palette=Category10[10])\n",
    "\n",
    "p = figure(title=\"umap dgits dataset\")\n",
    "p.circle(x='x',\n",
    "         y='y',\n",
    "         source=source,\n",
    "         color={\"field\": 'label', \"transform\": cmap},\n",
    "         legend='label')\n",
    "\n",
    "show(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
